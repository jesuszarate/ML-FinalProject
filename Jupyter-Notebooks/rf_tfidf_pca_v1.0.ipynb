{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as  np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "\n",
    "def dateToInt(x):    \n",
    "    _date = x.split('/')\n",
    "    \n",
    "    year, time = _date[2].split()\n",
    "    hour, minute = time.split(':')\n",
    "\n",
    "    if int(hour) > 23:\n",
    "        hour = 23\n",
    "    if int(hour) < 0:\n",
    "        hour = 0\n",
    "        \n",
    "    t = datetime.datetime(int(year), int(_date[0]), int(_date[1]), int(hour), int(minute), 0, 0).timestamp()\n",
    "    return t\n",
    "\n",
    "def getLabelMap(data):\n",
    "    # Map shape to an integer\n",
    "    labels = data['shape']\n",
    "    labels = labels.unique()\n",
    "    nums = [i for i in range(len(labels))]\n",
    "    return dict(zip(labels, nums))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize(text):    \n",
    "    # create the transform\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(text)\n",
    "\n",
    "    # encode document\n",
    "    vector = vectorizer.transform(text)\n",
    "\n",
    "    return vector.toarray()\n",
    "\n",
    "def vectorize_dm(text):\n",
    "    n_features = 5000\n",
    "    n_top_words = 20\n",
    "\n",
    "    count_vectorizor = CountVectorizer(\n",
    "            max_df=0.95, \n",
    "            min_df=2,\n",
    "            max_features=n_features,\n",
    "            stop_words='english'\n",
    "    )\n",
    "    count = count_vectorizor.fit_transform(text)\n",
    "#     count_feature_names = count_vectorizor.get_feature_names()\n",
    "#     print(count_feature_names)\n",
    "    return count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(text):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "    #     min_df=1,\n",
    "        max_df=0.7, \n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "#         stop_words=None,\n",
    "        stop_words='english')\n",
    "    vector = vectorizer.fit_transform(text)\n",
    "    # vectorizer.get_feature_names()\n",
    "    return vector.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['datetime', 'city', 'state', 'country',\n",
    "                              'shape', 'duration(seconds)', 'duration(hours/min)', 'comments',\n",
    "                              'date posted', 'longitude', 'latitude']\n",
    "data = pd.read_csv('../ufo-scrubbed-geocoded-time-standardized.csv',\n",
    "                   names=names,\n",
    "                   dtype=object,\n",
    "                   error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "data = data[data['country'] == 'us']\n",
    "\n",
    "# Columns currently not using\n",
    "names.remove('date posted')\n",
    "names.remove('duration(hours/min)')\n",
    "names.remove('country')\n",
    "names.remove('state')\n",
    "names.remove('city')\n",
    "\n",
    "# Make date into ints\n",
    "data['datetime'] = data.datetime.apply(lambda x: dateToInt(x))\n",
    "\n",
    "# Filter rows that have strings in the duration column\n",
    "data = data[data['duration(seconds)'].apply(lambda x : str.isdigit(x))]\n",
    "\n",
    "# Add keys to be able to merge\n",
    "data['key'] = [i for i in range(data.shape[0])]\n",
    "names.append('key')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Comment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64896, 24420)\n",
      "(64896, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get rid of unicode characters\n",
    "text = data[\"comments\"].apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in str(x)]))\n",
    "\n",
    "# Create dataframe out of the comment vector\n",
    "# commentDf = pd.DataFrame(vectorize_dm(text))\n",
    "commentDf = pd.DataFrame(tfidf(text))\n",
    "commentDf['key'] = [i for i in range(commentDf.shape[0])]\n",
    "print(commentDf.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Principle Component Anaysis (PCA) to only extract the 100 most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "dt = pca.fit_transform(commentDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64896, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the comment vectors with data\n",
    " Takes a  long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration(seconds)</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>key</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-638224200.0</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.9411111</td>\n",
       "      <td>0</td>\n",
       "      <td>32447.5</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>-0.048544</td>\n",
       "      <td>-0.093274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018625</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.018338</td>\n",
       "      <td>-0.012718</td>\n",
       "      <td>-0.031040</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>0.028385</td>\n",
       "      <td>-0.027876</td>\n",
       "      <td>-0.026148</td>\n",
       "      <td>-0.008617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-417297600.0</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.6458333</td>\n",
       "      <td>1</td>\n",
       "      <td>32446.5</td>\n",
       "      <td>-0.031388</td>\n",
       "      <td>-0.128362</td>\n",
       "      <td>-0.021686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024910</td>\n",
       "      <td>-0.018309</td>\n",
       "      <td>-0.020805</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>-0.021680</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.018363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-291070800.0</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.8036111</td>\n",
       "      <td>2</td>\n",
       "      <td>32445.5</td>\n",
       "      <td>-0.064271</td>\n",
       "      <td>-0.045229</td>\n",
       "      <td>-0.035193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>-0.015443</td>\n",
       "      <td>0.056996</td>\n",
       "      <td>-0.013207</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>-0.002535</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>-0.049151</td>\n",
       "      <td>0.096894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-259538400.0</td>\n",
       "      <td>sphere</td>\n",
       "      <td>300</td>\n",
       "      <td>36.5950000</td>\n",
       "      <td>-82.1888889</td>\n",
       "      <td>3</td>\n",
       "      <td>32444.5</td>\n",
       "      <td>-0.024537</td>\n",
       "      <td>-0.085541</td>\n",
       "      <td>-0.077092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>-0.015469</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>-0.018667</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>0.022287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-133294500.0</td>\n",
       "      <td>disk</td>\n",
       "      <td>1200</td>\n",
       "      <td>41.1175000</td>\n",
       "      <td>-73.4083333</td>\n",
       "      <td>4</td>\n",
       "      <td>32443.5</td>\n",
       "      <td>-0.037341</td>\n",
       "      <td>-0.011301</td>\n",
       "      <td>-0.013076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014662</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.052545</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>-0.007185</td>\n",
       "      <td>0.098949</td>\n",
       "      <td>-0.182941</td>\n",
       "      <td>0.049910</td>\n",
       "      <td>0.077155</td>\n",
       "      <td>0.111064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      datetime     shape duration(seconds)   longitude      latitude  key  \\\n",
       "0 -638224200.0  cylinder              2700  29.8830556   -97.9411111    0   \n",
       "1 -417297600.0    circle                20  28.9783333   -96.6458333    1   \n",
       "2 -291070800.0     light               900  21.4180556  -157.8036111    2   \n",
       "3 -259538400.0    sphere               300  36.5950000   -82.1888889    3   \n",
       "4 -133294500.0      disk              1200  41.1175000   -73.4083333    4   \n",
       "\n",
       "         0         1         2         3    ...           90        91  \\\n",
       "0  32447.5  0.022872 -0.048544 -0.093274    ...    -0.018625  0.019019   \n",
       "1  32446.5 -0.031388 -0.128362 -0.021686    ...    -0.024910 -0.018309   \n",
       "2  32445.5 -0.064271 -0.045229 -0.035193    ...     0.007032  0.026768   \n",
       "3  32444.5 -0.024537 -0.085541 -0.077092    ...     0.013916  0.007839   \n",
       "4  32443.5 -0.037341 -0.011301 -0.013076    ...    -0.014662  0.219529   \n",
       "\n",
       "         92        93        94        95        96        97        98  \\\n",
       "0  0.018338 -0.012718 -0.031040 -0.007422  0.028385 -0.027876 -0.026148   \n",
       "1 -0.020805  0.024780 -0.021680  0.029465  0.013871 -0.007990  0.022723   \n",
       "2 -0.015443  0.056996 -0.013207 -0.009036 -0.002535  0.001083 -0.049151   \n",
       "3 -0.010955 -0.015469  0.003070  0.006732 -0.018667 -0.003456 -0.007307   \n",
       "4  0.052545  0.027624 -0.007185  0.098949 -0.182941  0.049910  0.077155   \n",
       "\n",
       "         99  \n",
       "0 -0.008617  \n",
       "1  0.018363  \n",
       "2  0.096894  \n",
       "3  0.022287  \n",
       "4  0.111064  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we already vectorized the comments we can get rid of them now\n",
    "if 'comments' in names:\n",
    "    names.remove('comments')\n",
    "_dt = pd.DataFrame(dt)    \n",
    "_dt['key'] = [i for i in range(_dt.shape[0])]\n",
    "dt_combined = data[names].merge(_dt, on='key', how='left')\n",
    "dt_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Train Split\n",
    "\n",
    "For testing the classifiers we will split the data into a 'test' and 'train' set. This will be done with a 40-60% (test-train) split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = getLabelMap(dt_combined)\n",
    "\n",
    "x_cols = list(dt_combined)\n",
    "\n",
    "if 'shape' in x_cols:\n",
    "    x_cols.remove('shape')\n",
    "\n",
    "Y = np.array([label_dict[i] for i in dt_combined['shape']])    \n",
    "X = dt_combined[x_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2, \n",
    "    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=25)\n",
    "rf = RandomForestClassifier(n_estimators=25, max_features='auto', max_depth=None)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4682\n"
     ]
    }
   ],
   "source": [
    "# pred = rf.predict(X_test)\n",
    "y_pred_probs = rf.predict_proba(X_test)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "s = y_test\n",
    "count = 0\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == s[i]:\n",
    "        count += 1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and F Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36070878274268103"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scores = f1_score(y_test, y_guess, average=None)\n",
    "scores = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322256593672\n",
      "0.322256593672\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
